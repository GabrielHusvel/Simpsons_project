{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_5768\\600062901.py:34: DtypeWarning: Columns (4,5,6,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(\"../src/simpson_show_df.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantas chamadas ao LLM foram necessárias: 28\n",
      "Distribuição de fala por categoria: {'Positivas': 0, 'Neutras': 0, 'Negativas': 0, 'Negativa': 51, 'Neutra': 124, 'Positiva': 48}\n",
      "Relatório de avaliação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negativa       1.00      1.00      1.00         2\n",
      "      Neutra       1.00      1.00      1.00         2\n",
      "    Positiva       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         6\n",
      "   macro avg       1.00      1.00      1.00         6\n",
      "weighted avg       1.00      1.00      1.00         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Configurar API\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Exemplos para few-shot learning\n",
    "exemplos = \"\"\"\n",
    "Positivas:\n",
    "\"Terá todo o dinheiro do mundo, mas há algo que nunca poderá comprar ... um dinossauro!\"\n",
    "\"Olha para mim! Estou a fazer as pessoas felizes, sou o homem mágico, do país feliz, da casa da geleia, da rua do pirulito.\"\n",
    "\"Muito bom, ainda serve.\"\n",
    "\"É engraçado porque é verdade\"\n",
    "\"Adoro quando isso acontece.\"\n",
    "\n",
    "Negativas:\n",
    "\"Crianças, vocês tentaram muito. Para quê? Para fazer de si mesmo um tolo. A moral é: não se esforce.\"\n",
    "\"Sem TV e sem cerveja, Homer perde a cabeça.\"\n",
    "\"Se não gosta do seu trabalho, não faça greve, continue todos os dias e continue a fazê-lo a meio caminho. É assim que é americano!\"\n",
    "\"Não me coma, tenho mulher e filhos, coma-os.\"\n",
    "\"Não diga vingança ... não diga vingança ... Vingança!\"\n",
    "\n",
    "Neutras:\n",
    "\"E se cometermos um erro sobre religião? Deus ficaria mais furioso todas as semanas.\"\n",
    "\"Podes dizer-me onde está a pia? Gostaria de fingir que lavo as mãos.\"\n",
    "\"Salada não leva a nada.\"\n",
    "\"Queria que Deus estivesse vivo para ver isso.\"\n",
    "\"Estou lendo um livro interessante.\"\n",
    "\"\"\"\n",
    "\n",
    "# Carregar dataset\n",
    "dataset = pd.read_csv(\"../data/simpson_show_df.csv\")\n",
    "episodio_92_temporada_5 = dataset[(dataset['episode_id'] == 92) & (dataset['episode_season'] == 5)]\n",
    "falas_episodio = episodio_92_temporada_5['spoken_words'].tolist()\n",
    "\n",
    "# Dividir falas em lotes \n",
    "batch_size = 10\n",
    "lotes = [falas_episodio[i:i + batch_size] for i in range(0, len(falas_episodio), batch_size)]\n",
    "\n",
    "# Normalizar rótulos de saída\n",
    "def normalizar_rotulo(rotulo):\n",
    "    rotulo = rotulo.strip().lower()\n",
    "    if \"positiva\" in rotulo:\n",
    "        return \"Positiva\"\n",
    "    elif \"negativa\" in rotulo:\n",
    "        return \"Negativa\"\n",
    "    elif \"neutra\" in rotulo:\n",
    "        return \"Neutra\"\n",
    "    else:\n",
    "        return \"Neutra\"  \n",
    "\n",
    "# few-shot learning\n",
    "resultados = []\n",
    "for lote in lotes:\n",
    "    falas_texto = \"\\n\".join([f\"Fala: \\\"{fala}\\\"\" for fala in lote])\n",
    "    prompt = f\"\"\"\n",
    "    Você é um especialista em análise de sentimentos. Baseie-se nos exemplos a seguir:\n",
    "    {exemplos}\n",
    "    Classifique as falas abaixo como Positivas, Neutras ou Negativas:\n",
    "    {falas_texto}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=150,\n",
    "        temperature=1,\n",
    "    )\n",
    "    \n",
    "    classificacoes = [normalizar_rotulo(c.strip()) for c in response.choices[0].text.strip().split('\\n')]\n",
    "    resultados.extend(classificacoes)\n",
    "\n",
    "# Quantidade de chamadas ao LLM\n",
    "chamadas_llm = len(lotes)\n",
    "\n",
    "distribuicao = {\"Positivas\": 0, \"Neutras\": 0, \"Negativas\": 0}\n",
    "for resultado in resultados:\n",
    "    if resultado not in distribuicao:\n",
    "        distribuicao[resultado] = 0 \n",
    "    distribuicao[resultado] += 1\n",
    "\n",
    "\n",
    "# Avaliar o modelo com subconjunto de validação\n",
    "falas_validacao = [\n",
    "    \"Isso é ótimo!\", \"Horrível, não suporto mais isso.\", \"Apenas mais um dia comum.\",\n",
    "    \"Que incrível resultado!\", \"Essa foi péssima.\", \"Nada de especial aconteceu hoje.\"\n",
    "]\n",
    "labels_verdadeiros = [\"Positiva\", \"Negativa\", \"Neutra\", \"Positiva\", \"Negativa\", \"Neutra\"]\n",
    "\n",
    "# Criar prompt para validação\n",
    "falas_validacao_prompt = \"\\n\".join([f\"Fala: \\\"{fala}\\\"\" for fala in falas_validacao])\n",
    "prompt_validacao = f\"\"\"\n",
    "Você é um especialista em análise de sentimentos. Baseie-se nos exemplos a seguir:\n",
    "{exemplos}\n",
    "Classifique as falas abaixo como Positivas, Neutras ou Negativas:\n",
    "{falas_validacao_prompt}\n",
    "\"\"\"             \n",
    "\n",
    "response_validacao = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt_validacao,\n",
    "    max_tokens=150,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "all_labels = set(labels_verdadeiros + [normalizar_rotulo(res.strip()) for res in response_validacao.choices[0].text.strip().split('\\n')])\n",
    "\n",
    "# Avaliação do modelo\n",
    "relatorio = classification_report(\n",
    "    labels_verdadeiros,\n",
    "    [normalizar_rotulo(res.strip()) for res in response_validacao.choices[0].text.strip().split('\\n')],\n",
    "    target_names=list(all_labels),  \n",
    "    zero_division=0 \n",
    ")\n",
    "\n",
    "# Resultados\n",
    "print(\"Quantas chamadas ao LLM foram necessárias:\", chamadas_llm)\n",
    "print(\"Distribuição de fala por categoria:\", distribuicao)\n",
    "print(\"Relatório de avaliação:\\n\", relatorio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_5768\\4055441034.py:2: DtypeWarning: Columns (4,5,6,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(\"../src/simpson_show_df.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Carregar dataset\n",
    "dataset = pd.read_csv(\"../data/simpson_show_df.csv\")\n",
    "episodio_92_temporada_5 = dataset[(dataset['episode_id'] == 92) & (dataset['episode_season'] == 5)]\n",
    "falas_episodio = episodio_92_temporada_5['spoken_words'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar tamanhos para serem iguais\n",
    "min_tamanho = min(len(falas_episodio), len(resultados))\n",
    "\n",
    "falas_episodioo = falas_episodio[:min_tamanho]\n",
    "resultadoss = resultados[:min_tamanho]\n",
    "\n",
    "df_resultados = pd.DataFrame({\n",
    "    \"Fala\": falas_episodioo,\n",
    "    \"Categoria\": resultadoss\n",
    "})\n",
    "\n",
    "df_resultados.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_resultados.to_csv(\"ep92se5_classification.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho de falas_episodio: 277\n",
      "Tamanho de resultados: 223\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho de falas_episodio:\", len(falas_episodio))\n",
    "print(\"Tamanho de resultados:\", len(resultados))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote 1: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 2: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 3: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 4: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 5: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 6: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 7: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 8: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 9: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 10: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 11: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 12: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 13: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 14: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 15: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 16: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 17: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 18: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 19: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 20: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 21: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 22: 10 falas\n",
      "Classificações retornadas para o lote: 10\n",
      "Lote 23: 10 falas\n",
      "Classificações retornadas para o lote: 3\n",
      "Lote 24: 10 falas\n",
      "Classificações retornadas para o lote: 0\n",
      "Lote 25: 10 falas\n",
      "Classificações retornadas para o lote: 0\n",
      "Lote 26: 10 falas\n",
      "Classificações retornadas para o lote: 0\n",
      "Lote 27: 10 falas\n",
      "Classificações retornadas para o lote: 0\n",
      "Lote 28: 7 falas\n",
      "Classificações retornadas para o lote: 0\n"
     ]
    }
   ],
   "source": [
    "for i, lote in enumerate(lotes):\n",
    "    print(f\"Lote {i + 1}: {len(lote)} falas\"\n",
    "    print(f\"Classificações retornadas para o lote: {len(resultados[i * batch_size: (i + 1) * batch_size])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
